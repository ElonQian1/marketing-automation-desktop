// src-tauri/src/core/adapters/outbound/ai_agent/openai_compatible_provider.rs
// module: core/adapters/outbound/ai_agent | layer: adapters | role: openai-provider
// summary: OpenAI å…¼å®¹ API æä¾›å•†å®ç° - æ”¯æŒ OpenAIã€æ··å…ƒã€DeepSeek ç­‰

use async_trait::async_trait;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use serde_json::{json, Value};
use tracing::{info, warn, error, debug};

use crate::core::domain::agent::{
    AgentMessage, AgentTool, AiProvider, AiProviderConfig,
    MessageRole, ToolCall, FunctionCall,
};
use crate::core::shared::{CoreError, CoreResult};

/// OpenAI å…¼å®¹ API æä¾›å•†
/// 
/// æ”¯æŒæ‰€æœ‰ OpenAI API å…¼å®¹çš„æœåŠ¡ï¼š
/// - OpenAI
/// - è…¾è®¯æ··å…ƒ
/// - DeepSeek
/// - Azure OpenAI
/// - æœ¬åœ° Ollama ç­‰
pub struct OpenAiCompatibleProvider {
    config: AiProviderConfig,
    client: Client,
}

impl OpenAiCompatibleProvider {
    /// åˆ›å»ºæ–°çš„æä¾›å•†å®ä¾‹
    pub fn new(config: AiProviderConfig) -> Self {
        let client = Client::builder()
            .timeout(std::time::Duration::from_secs(120))
            .build()
            .expect("Failed to create HTTP client");
        
        Self { config, client }
    }

    /// æ„å»ºè¯·æ±‚ body
    fn build_request_body(
        &self,
        messages: &[AgentMessage],
        tools: Option<&[AgentTool]>,
    ) -> Value {
        let mut body = json!({
            "model": self.config.model,
            "messages": messages.iter().map(|m| self.message_to_json(m)).collect::<Vec<_>>(),
        });

        if let Some(max_tokens) = self.config.max_tokens {
            body["max_tokens"] = json!(max_tokens);
        }

        if let Some(temperature) = self.config.temperature {
            body["temperature"] = json!(temperature);
        }

        if let Some(tools) = tools {
            if !tools.is_empty() {
                body["tools"] = json!(tools);
                body["tool_choice"] = json!("auto");
            }
        }

        body
    }

    /// å°† AgentMessage è½¬æ¢ä¸º JSON
    fn message_to_json(&self, message: &AgentMessage) -> Value {
        let mut msg = json!({
            "role": match message.role {
                MessageRole::System => "system",
                MessageRole::User => "user",
                MessageRole::Assistant => "assistant",
                MessageRole::Tool => "tool",
            },
            "content": message.content,
        });

        // æ·»åŠ å·¥å…·è°ƒç”¨
        if let Some(tool_calls) = &message.tool_calls {
            msg["tool_calls"] = json!(tool_calls.iter().map(|tc| {
                json!({
                    "id": tc.id,
                    "type": tc.call_type,
                    "function": {
                        "name": tc.function.name,
                        "arguments": tc.function.arguments,
                    }
                })
            }).collect::<Vec<_>>());
        }

        // æ·»åŠ å·¥å…·è°ƒç”¨ ID
        if let Some(tool_call_id) = &message.tool_call_id {
            msg["tool_call_id"] = json!(tool_call_id);
        }

        msg
    }

    /// è§£æå“åº”
    fn parse_response(&self, response: &ChatCompletionResponse) -> CoreResult<AgentMessage> {
        let choice = response.choices.first()
            .ok_or_else(|| CoreError::external_service("AI è¿”å›ç©ºå“åº”"))?;

        let message = &choice.message;

        // æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if let Some(tool_calls) = &message.tool_calls {
            if !tool_calls.is_empty() {
                let parsed_calls: Vec<ToolCall> = tool_calls.iter().map(|tc| {
                    ToolCall {
                        id: tc.id.clone(),
                        call_type: tc.r#type.clone(),
                        function: FunctionCall {
                            name: tc.function.name.clone(),
                            arguments: tc.function.arguments.clone(),
                        },
                    }
                }).collect();

                return Ok(AgentMessage::assistant_with_tools(
                    message.content.clone(),
                    parsed_calls,
                ));
            }
        }

        // æ™®é€šæ–‡æœ¬å›å¤
        Ok(AgentMessage::assistant(
            message.content.clone().unwrap_or_default()
        ))
    }
}

#[async_trait]
impl AiProvider for OpenAiCompatibleProvider {
    fn name(&self) -> &str {
        &self.config.name
    }

    fn config(&self) -> &AiProviderConfig {
        &self.config
    }

    async fn chat(&self, messages: Vec<AgentMessage>) -> CoreResult<AgentMessage> {
        self.chat_with_tools(messages, vec![]).await
    }

    async fn chat_with_tools(
        &self,
        messages: Vec<AgentMessage>,
        tools: Vec<AgentTool>,
    ) -> CoreResult<AgentMessage> {
        let url = format!("{}/chat/completions", self.config.base_url);
        
        let body = self.build_request_body(
            &messages,
            if tools.is_empty() { None } else { Some(&tools) },
        );

        debug!("ğŸ¤– å‘é€ AI è¯·æ±‚åˆ° {}: {:?}", self.config.name, body);

        let response = self.client
            .post(&url)
            .header("Authorization", format!("Bearer {}", self.config.api_key))
            .header("Content-Type", "application/json")
            .json(&body)
            .send()
            .await
            .map_err(|e| CoreError::external_service(format!("è¯·æ±‚å¤±è´¥: {}", e)))?;

        let status = response.status();
        
        if !status.is_success() {
            let error_text = response.text().await.unwrap_or_default();
            error!("âŒ AI API é”™è¯¯ [{}]: {}", status, error_text);
            return Err(CoreError::external_service(format!(
                "AI API é”™è¯¯ [{}]: {}",
                status, error_text
            )));
        }

        let response_body: ChatCompletionResponse = response.json().await
            .map_err(|e| CoreError::external_service(format!("è§£æå“åº”å¤±è´¥: {}", e)))?;

        debug!("ğŸ“¥ AI å“åº”: {:?}", response_body);

        // è®°å½• token ä½¿ç”¨
        if let Some(usage) = &response_body.usage {
            info!(
                "ğŸ“Š Token ä½¿ç”¨: prompt={}, completion={}, total={}",
                usage.prompt_tokens, usage.completion_tokens, usage.total_tokens
            );
        }

        self.parse_response(&response_body)
    }
}

// ============================================================================
// OpenAI API å“åº”ç»“æ„
// ============================================================================

#[derive(Debug, Deserialize)]
struct ChatCompletionResponse {
    id: String,
    choices: Vec<Choice>,
    usage: Option<Usage>,
}

#[derive(Debug, Deserialize)]
struct Choice {
    index: u32,
    message: ResponseMessage,
    finish_reason: Option<String>,
}

#[derive(Debug, Deserialize)]
struct ResponseMessage {
    role: String,
    content: Option<String>,
    tool_calls: Option<Vec<ResponseToolCall>>,
}

#[derive(Debug, Deserialize)]
struct ResponseToolCall {
    id: String,
    #[serde(rename = "type")]
    r#type: String,
    function: ResponseFunction,
}

#[derive(Debug, Deserialize)]
struct ResponseFunction {
    name: String,
    arguments: String,
}

#[derive(Debug, Deserialize)]
struct Usage {
    prompt_tokens: u32,
    completion_tokens: u32,
    total_tokens: u32,
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_config_creation() {
        let config = AiProviderConfig::hunyuan("test-key");
        assert_eq!(config.name, "è…¾è®¯æ··å…ƒ");
        assert!(config.base_url.contains("hunyuan"));
    }
}
